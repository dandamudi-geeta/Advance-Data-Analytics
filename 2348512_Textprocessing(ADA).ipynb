{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YzZXGSQhiv7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer, RegexpTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "5O1J3uuKi_WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Hey there! ðŸ˜ƒ I can't believe it's already 2024. Did you see John's new blog post? Check it out at https://example.com/blog! Also, email me at john.doe@example.com. He mentioned something about stemming and lemmatizationâ€”interesting stuff. BTW, I'll be attending the AI conference in N.Y.C. next month!! #Excited #AI ðŸ˜Š Let's catch up soon. Cheers, John\""
      ],
      "metadata": {
        "id": "cMOBvRZvoHgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lowercase\n",
        "#remove numbers\n",
        "#special characters\n",
        "#punctuations\n",
        "#url\n",
        "#expansion of shortwords"
      ],
      "metadata": {
        "id": "Hp8q7XeGkBzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26NhL3QGlrEq",
        "outputId": "8d6d7bd5-5a65-4dac-816e-d3a8a6eda52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Za3uePcKlnXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the paragraph using spaCy\n",
        "doc = nlp(paragraph)"
      ],
      "metadata": {
        "id": "Exl9mpY4l3f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract words from the processed document\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "# Display the result\n",
        "print(tokens)\n",
        "for i in range(0,len(tokens)):\n",
        "  print(tokens[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyM6RvYJmAWN",
        "outputId": "7a9f39fa-878e-4cff-a2ec-83aa643a13a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', 'there', '!', 'ðŸ˜ƒ', 'I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', '2024', '.', 'Did', 'you', 'see', 'John', \"'s\", 'new', 'blog', 'post', '?', 'Check', 'it', 'out', 'at', 'https://example.com/blog', '!', 'Also', ',', 'email', 'me', 'at', 'john.doe@example.com', '.', 'He', 'mentioned', 'something', 'about', 'stemming', 'and', 'lemmatization', '-', 'interesting', 'stuff', '.', 'BTW', ',', 'I', 'will', 'be', 'attending', 'the', 'AI', 'conference', 'in', 'N.Y.C.', 'next', 'month', '!', '!', '#', 'Excited', '#', 'AI', 'ðŸ˜Š', 'Let', \"'s\", 'catch', 'up', 'soon', '.', 'Cheers', ',', 'John', '.']\n",
            "Hey\n",
            "there\n",
            "!\n",
            "ðŸ˜ƒ\n",
            "I\n",
            "ca\n",
            "n't\n",
            "believe\n",
            "it\n",
            "'s\n",
            "already\n",
            "2024\n",
            ".\n",
            "Did\n",
            "you\n",
            "see\n",
            "John\n",
            "'s\n",
            "new\n",
            "blog\n",
            "post\n",
            "?\n",
            "Check\n",
            "it\n",
            "out\n",
            "at\n",
            "https://example.com/blog\n",
            "!\n",
            "Also\n",
            ",\n",
            "email\n",
            "me\n",
            "at\n",
            "john.doe@example.com\n",
            ".\n",
            "He\n",
            "mentioned\n",
            "something\n",
            "about\n",
            "stemming\n",
            "and\n",
            "lemmatization\n",
            "-\n",
            "interesting\n",
            "stuff\n",
            ".\n",
            "BTW\n",
            ",\n",
            "I\n",
            "will\n",
            "be\n",
            "attending\n",
            "the\n",
            "AI\n",
            "conference\n",
            "in\n",
            "N.Y.C.\n",
            "next\n",
            "month\n",
            "!\n",
            "!\n",
            "#\n",
            "Excited\n",
            "#\n",
            "AI\n",
            "ðŸ˜Š\n",
            "Let\n",
            "'s\n",
            "catch\n",
            "up\n",
            "soon\n",
            ".\n",
            "Cheers\n",
            ",\n",
            "John\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import string"
      ],
      "metadata": {
        "id": "ary1gIVYncsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifWAj4hsndHd",
        "outputId": "b754fe34-c59e-42c2-8bc9-109d088f7d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.12.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting text to LowerCase**\n"
      ],
      "metadata": {
        "id": "2ROu8i0knsS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting text to lowercase\n",
        "paragraph_lower = text.lower()\n",
        "print(paragraph_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2_bkOfznfFj",
        "outputId": "0010ada9-e2db-47e8-8142-82b6ad20c13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey there! ðŸ˜ƒ i can't believe it's already 2024. did you see john's new blog post? check it out at https://example.com/blog! also, email me at john.doe@example.com. he mentioned something about stemming and lemmatizationâ€”interesting stuff. btw, i'll be attending the ai conference in n.y.c. next month!! #excited #ai ðŸ˜Š let's catch up soon. cheers, john\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting into tokens**"
      ],
      "metadata": {
        "id": "QhVYp7yUoZJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting into tokens\n",
        "\n",
        "tokens=paragraph_lower.split()\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-Vcqudjn0xt",
        "outputId": "5bf93a0a-d42f-4aee-ad5a-8749c3373438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hey', 'there!', 'ðŸ˜ƒ', 'i', \"can't\", 'believe', \"it's\", 'already', '2024.', 'did', 'you', 'see', \"john's\", 'new', 'blog', 'post?', 'check', 'it', 'out', 'at', 'https://example.com/blog!', 'also,', 'email', 'me', 'at', 'john.doe@example.com.', 'he', 'mentioned', 'something', 'about', 'stemming', 'and', 'lemmatizationâ€”interesting', 'stuff.', 'btw,', \"i'll\", 'be', 'attending', 'the', 'ai', 'conference', 'in', 'n.y.c.', 'next', 'month!!', '#excited', '#ai', 'ðŸ˜Š', \"let's\", 'catch', 'up', 'soon.', 'cheers,', 'john']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing punctuation**"
      ],
      "metadata": {
        "id": "GNP1xo6pokC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing punctuation\n",
        "\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42vqO8ZqoRLm",
        "outputId": "43239cba-3931-4c08-b658-9962535280e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey there ðŸ˜ƒ I cant believe its already 2024 Did you see Johns new blog post Check it out at httpsexamplecomblog Also email me at johndoeexamplecom He mentioned something about stemming and lemmatizationâ€”interesting stuff BTW Ill be attending the AI conference in NYC next month Excited AI ðŸ˜Š Lets catch up soon Cheers John\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing the URLS**"
      ],
      "metadata": {
        "id": "vJsS-egOo3YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the urls\n",
        "text = re.sub(r'http\\S+', '', text)\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIJebn2iouB0",
        "outputId": "b30a92d9-7997-459c-88c2-d79e734bb5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey there ðŸ˜ƒ I cant believe its already 2024 Did you see Johns new blog post Check it out at  Also email me at johndoeexamplecom He mentioned something about stemming and lemmatizationâ€”interesting stuff BTW Ill be attending the AI conference in NYC next month Excited AI ðŸ˜Š Lets catch up soon Cheers John\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing the emoji"
      ],
      "metadata": {
        "id": "reUcuYlPo_6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "text = emoji.replace_emoji(text, replace='')\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWm2T76uozWB",
        "outputId": "f217e5a4-b6b9-4923-8fea-f9d371893ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey there  I cant believe its already 2024 Did you see Johns new blog post Check it out at  Also email me at johndoeexamplecom He mentioned something about stemming and lemmatizationâ€”interesting stuff BTW Ill be attending the AI conference in NYC next month Excited AI  Lets catch up soon Cheers John\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qci3crPrpF2P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}